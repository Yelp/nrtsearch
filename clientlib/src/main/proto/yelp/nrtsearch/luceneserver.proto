/* Description of NRTSearch Service APIs and message types */
syntax = "proto3";

import "yelp/nrtsearch/search.proto";
import "yelp/nrtsearch/analysis.proto";
import "google/api/annotations.proto";
import "google/api/httpbody.proto";
import "google/protobuf/empty.proto";
import "google/protobuf/struct.proto";


option java_multiple_files = true;
option java_package = "com.yelp.nrtsearch.server.grpc";
option java_outer_classname = "LuceneServerProto";
option objc_class_prefix = "HLW";

package luceneserver;

// The LuceneServer service definition.
service LuceneServer {
    /* Create an Index */
    rpc createIndex (CreateIndexRequest) returns (CreateIndexResponse) {
        option (google.api.http) = {
      post: "/v1/create_index"
      body: "*"
    };

    }
    /* Change global offline or online settings for this index. */
    rpc liveSettings (LiveSettingsRequest) returns (LiveSettingsResponse) {
        option (google.api.http) = {
      post: "/v1/live_settings"
      body: "*"
    };

    }
    /* Registers one or more fields.  Fields must be registered before they can be added in a document (via @addDocument).
      Pass a list of Fields and an indexName. Any number of fields may be registered in a single request,
      and once a field is registered it cannot be changed (write-once).
      This returns the full set of fields currently registered. */
    rpc registerFields (FieldDefRequest) returns (FieldDefResponse) {
        option (google.api.http) = {
      post: "/v1/register_fields"
      body: "*"
    };

    }

    /* Adds one or more fields.  Fields must be registered before they can be added in a document (via @addDocument).
  Pass a list of Fields and an indexName. Any number of fields may be registered in a single request,
  and once a field is registered it cannot be changed (write-once).
  This returns the full set of fields currently registered. */
    rpc updateFields (FieldDefRequest) returns (FieldDefResponse) {
        option (google.api.http) = {
      post: "/v1/update_fields"
      body: "*"
    };

    }

    /* Change global offline settings for this index.
    This returns the currently set settings; pass no settings changes to retrieve current settings.*/
    rpc settings (SettingsRequest) returns (SettingsResponse) {
        option (google.api.http) = {
      post: "/v1/settings"
      body: "*"
    };

    }
    /* Starts an index */
    rpc startIndex (StartIndexRequest) returns (StartIndexResponse) {
        option (google.api.http) = {
      post: "/v1/start_index"
      body: "*"
    };

    }
    /* Stops an index */
    rpc stopIndex (StopIndexRequest) returns (DummyResponse) {
        option (google.api.http) = {
      post: "/v1/stop_index"
      body: "*"
    };

    }
    /* Adds a stream of Documents */
    rpc addDocuments (stream AddDocumentRequest) returns (AddDocumentResponse) {
        option (google.api.http) = {
      post: "/v1/add_documents"
      body: "*"
    };

    }
    /* Refresh the latest searcher for an index */
    rpc refresh (RefreshRequest) returns (RefreshResponse) {
        option (google.api.http) = {
      post: "/v1/refresh"
      body: "*"
    };

    }
    /* Commits all pending changes to durable storage*/
    rpc commit (CommitRequest) returns (CommitResponse) {
        option (google.api.http) = {
      post: "/v1/commit"
      body: "*"
    };

    }
    /* Retrieve index statistics*/
    rpc stats (StatsRequest) returns (StatsResponse) {
        option (google.api.http) = {
      post: "/v1/stats"
      body: "*"
      additional_bindings {
        get: "/v1/stats/{indexName}"
      }
    };

    }
    /* Search */
    rpc search (SearchRequest) returns (SearchResponse) {
        option (google.api.http) = {
      post: "/v1/search"
      body: "*"
    };
    }
    /* Delete documents */
    rpc delete (AddDocumentRequest) returns (AddDocumentResponse) {
        option (google.api.http) = {
      post: "/v1/delete"
      body: "*"
    };

    }
    /* Delete documents matching a query */
    rpc deleteByQuery (DeleteByQueryRequest) returns (AddDocumentResponse) {
        option (google.api.http) = {
      post: "/v1/delete_by_query"
      body: "*"
    };

    }
    /* Delete all documents for index */
    rpc deleteAll (DeleteAllDocumentsRequest) returns (DeleteAllDocumentsResponse) {
        option (google.api.http) = {
      post: "/v1/delete_all"
      body: "*"
    };

    }
    /* Delete index */
    rpc deleteIndex (DeleteIndexRequest) returns (DeleteIndexResponse) {
        option (google.api.http) = {
      post: "/v1/delete_index"
      body: "*"
    };

    }

    /* Builds a new auto-suggester, loading suggestions via the provided local file path.*/
    rpc buildSuggest (BuildSuggestRequest) returns (BuildSuggestResponse) {
        option (google.api.http) = {
      post: "/v1/suggest_build"
      body: "*"
    };

    }
    /* Perform an auto-suggest lookup.*/
    rpc suggestLookup (SuggestLookupRequest) returns (SuggestLookupResponse) {
        option (google.api.http) = {
      post: "/v1/suggest_lookup"
      body: "*"
    };

    }
    /* Updates existing suggestions, if the suggester supports near-real-time changes. */
    rpc updateSuggest (BuildSuggestRequest) returns (BuildSuggestResponse) {
        option (google.api.http) = {
      post: "/v1/suggest_update"
      body: "*"
    };

    }

    /*
    Creates a snapshot in the index, which is saved point-in-time view of the last commit
    in the index such that no files referenced by that snapshot will be deleted by ongoing
    indexing until the snapshot is released with @releaseSnapshot.  Note that this will
    reference the last commit, so be sure to call commit first if you have pending changes
    that you'd like to be included in the snapshot.
    This can be used for backup purposes, i.e. after creating the snapshot you can copy
    all referenced files to backup storage, and then release the snapshot once complete.
    To restore the backup, just copy all the files back and restart the server.
    It can also be used for transactional purposes, i.e. if you sometimes need to search a
    specific snapshot instead of the current live index. Creating a snapshot is very fast
    (does not require any file copying), but over time it will consume extra disk space as
    old segments are merged in the index.  Be sure to release the snapshot once you're done.
    Snapshots survive shutdown and restart of the server.  Returns all protected filenames
    referenced by this snapshot: these files will not change and will not be deleted until
    the snapshot is released.  This returns the directories and files referenced by the snapshot.
    */
    rpc createSnapshot (CreateSnapshotRequest) returns (CreateSnapshotResponse) {
        option (google.api.http) = {
      post: "/v1/create_snapshot"
      body: "*"
    };

    }

    /* releases a snapshot previously created with @createSnapshot. */
    rpc releaseSnapshot (ReleaseSnapshotRequest) returns (ReleaseSnapshotResponse) {
        option (google.api.http) = {
      post: "/v1/release_snapshot"
      body: "*"
    };

    }

    /* Gets all unreleased index gens of snapshots previously created with @createSnapshot. */
    rpc getAllSnapshotIndexGen (GetAllSnapshotGenRequest) returns (GetAllSnapshotGenResponse) {
        option (google.api.http) = {
      get: "/v1/get_all_snapshot_index_gen/{indexName}"
    };

    }

    /* backs up a resource (index) and it associated metadata e.g. settings, schema to s3 */
    rpc backupIndex (BackupIndexRequest) returns (BackupIndexResponse) {
        option (google.api.http) = {
      post: "/v1/backup_index"
      body: "*"
    };
    }
    /* Gets the state of a started index, includes settings, live_settings, search schema, suggest schema */
    rpc state (StateRequest) returns (StateResponse) {
        option (google.api.http) = {
        post: "/v1/state"
        body: "*"
        additional_bindings {
            get: "/v1/state/{indexName}"
        }
    };
    }

    //GET methods
    /* healthcheck */
    rpc status (HealthCheckRequest) returns (HealthCheckResponse) {
        option (google.api.http) = {
      get: "/v1/status"
    };

    }
    /* metrics  */
    rpc metrics (google.protobuf.Empty) returns (google.api.HttpBody) {
        option (google.api.http) = {
      get: "/status/metrics"
    };
    }
    /* indices  */
    rpc indices (IndicesRequest) returns (IndicesResponse) {
        option (google.api.http) = {
      get: "/v1/indices"
    };
    }

    rpc forceMerge(ForceMergeRequest) returns (ForceMergeResponse) {
        option (google.api.http) = {
      post: "/v1/force_merge"
      body: "*"
    };
    }
}

//The ReplicationServer service definition.
service ReplicationServer {
    /* Issued by replica on primary node when it comes up */
    rpc addReplicas (AddReplicaRequest) returns (AddReplicaResponse) {
    }
    /* Issued by replica to receive CopyState from primary */
    rpc recvCopyState (CopyStateRequest) returns (CopyState) {
    }
    /* Send a file as a stream in chunks*/
    rpc sendRawFile (stream RawFileChunk) returns (TransferStatus) {
    }
    /* Receives a file as a stream in chunks. Typically issued by replica on primary */
    rpc recvRawFile (FileInfo) returns (stream RawFileChunk) {
    }
    /* Issued by primary on replica to inform it to start copying files either pre-warming (new merged segments) or when replica comes up first time */
    rpc copyFiles (CopyFiles) returns (stream TransferStatus) {
    }
    /* Invoked externally to replica, to notify it that a new NRT point was just created on the primary */
    rpc newNRTPoint (NewNRTPoint) returns (TransferStatus) {
    }
    /** Invoked externally to primary, to make all recent index operations searchable on the primary and, once copying is done, on the replicas */
    rpc writeNRTPoint (IndexName) returns (SearcherVersion) {
    }
    /** Invoked externally to replica, to get the current Searcher version on replica.*/
    rpc getCurrentSearcherVersion (IndexName) returns (SearcherVersion) {
    }
    /** Invoked externally on primary to find the list of replica nodes this node is connected to for binary replication per index */
    rpc getConnectedNodes (GetNodesRequest) returns (GetNodesResponse) {
    }

}

/* Input to createIndex */
message CreateIndexRequest {
    string indexName = 1; // name of the index to be created. [a-zA-Z0-9]*
    string rootDir = 2; //rootDirectory that maintains all state files for durability.
}

/* Response from Server to createIndex */
message CreateIndexResponse {
    string response = 1;
}

/* Input to liveSettings */
message LiveSettingsRequest {
    string indexName = 1; // name of index whose liveSettings are to be updated.
    //Longest time to wait before reopening IndexSearcher (i.e., periodic background reopen).
    double maxRefreshSec = 2;
    //Shortest time to wait before reopening IndexSearcher (i.e., when a search is waiting for a specific indexGen).
    double minRefreshSec = 3;
    //Non-current searchers older than this are pruned.
    double maxSearcherAgeSec = 4;
    //Size (in MB) of IndexWriter's RAM buffer.
    double indexRamBufferSizeMB = 5;
    //Max number of documents to add at a time.
    int32 addDocumentsMaxBufferLen = 6;
}

/* Response from Server to liveSettings */
message LiveSettingsResponse {
    string response = 1;
}

//Type of the field
enum FieldType {
    ATOM = 0; // Text that's indexed as a single token, with DOCS_ONLY and omitting norms.
    TEXT = 1; // Text that's tokenized and indexed, with the index-time analyzer.
    BOOLEAN = 2; // Boolean value.
    LONG = 3; //Long value.
    INT = 4; // Int value.
    DOUBLE = 5; //Double value.
    FLOAT = 6; // Float value.
    LAT_LON = 7; // A latitude/longitude point.
    DATE_TIME = 8; // Date and optional time.
    // TODO name this "dynamic" instead of "virtual"?
    VIRTUAL = 9; // Virtual field defined with a JavaScript expression.
    // TODO need tests for internal:
    INTERNAL = 10; //Internal field, currently only for holding indexed facets data.
    CUSTOM = 11; // Field type specified by name.
    _ID = 12; // Field which will be used as document IDs
}

//How the tokens should be indexed.
enum IndexOptions {
    DOCS_FREQS_POSITIONS = 0; // Index doc ids, term frequencies and positions.
    DOCS = 1; // Index only doc ids (for binary search).
    DOCS_FREQS = 2; // Index doc ids and term frequencies.
    DOCS_FREQS_POSITIONS_OFFSETS = 3; // Index doc ids, term frequencies, positions and offsets.
}

//Whether/how term vectors should be indexed.
enum TermVectors {
    NO_TERMVECTORS = 0; // no term vectors are indexed
    TERMS = 1; // Index terms and freqs only.
    TERMS_POSITIONS = 2; // Index terms, freqs and positions.
    TERMS_POSITIONS_OFFSETS = 3; // Index terms, freqs, positions and offsets.
    TERMS_POSITIONS_OFFSETS_PAYLOADS = 4; // Index terms, freqs, positions, offsets and payloads
}

//Whether/How this field should index facets, and how.
enum FacetType {
    NO_FACETS = 0; //No facets are indexed.
    FLAT = 1; //Facets are indexed with no hierarchy.
    HIERARCHY = 2; //Facets are indexed and are hierarchical.
    NUMERIC_RANGE = 3; //Compute facet counts for custom numeric ranges
    SORTED_SET_DOC_VALUES = 4; //Uses SortedSetDocValuesFacetCounts, which must be flat but don't require a taxonomy index
}

message Field {
    string name = 1; // name of the field
    FieldType type = 2;
    bool search = 3; // True if the value should be available for searching (or numeric range searching, for a numeric field).
    bool store = 4; // True if the value should be stored.
    bool storeDocValues = 5; // Whether to index the value into doc values.
    bool sort = 6; // True if the value should be indexed into doc values for sorting.
    bool tokenize = 7; // True if the value should be tokenized.
    bool group = 8; // True if the value should be indexed into doc values for grouping.
    bool multiValued = 9; // True if this field may sometimes have more than one value.
    bool highlight = 10; // True if the value should be indexed for highlighting.
    bool omitNorms = 11; // True if norms are omitted.
    string dateTimeFormat = 12; // Format string used to parse datetime fields
    string postingsFormat = 13; // Which PostingsFormat should be used to index this field.
    string docValuesFormat = 14; // Which DocValuesFormat should be used to index this field.
    IndexOptions indexOptions = 15; //How the tokens should be indexed.
    Script script = 16; // The script definition defining a virtual field's value (only used with type=virtual).
    //TODO make analyzers message types i.e. StandardAnalyzer, EnglishAnalyzer, CustomAnalyzer etc
    Analyzer analyzer = 17; // Analyzer to use for this field during indexing and searching.
    Analyzer indexAnalyzer = 18; // Analyzer to use for this field during indexing.
    Analyzer searchAnalyzer = 19; //Analyzer to use for this field during searching.
    TermVectors termVectors = 20; // Whether/how term vectors should be indexed.
    //TODO make similarity message types i.d. DefaultSimilarity, CustomSimilarity, BM25Similarity;
    string similarity = 21; // Which Similarity implementation to use for this field.
    FacetType facet = 22; // Whether this field should index facets, and how.
    string facetIndexFieldName = 23; // "Which underlying Lucene index field is used to hold any indexed taxonomy or sorted set doc values facets
    google.protobuf.Struct additionalProperties = 24; // Additional info needed to configure field, used for CUSTOM types.
}

/* Input to registerFields */
message FieldDefRequest {
    string indexName = 1; // name of the index against which the field is to be created
    repeated Field field = 2;
}

/* Response from Server for registerFields */
message FieldDefResponse {
    string response = 1;
}

/* Input to settings */
message SettingsRequest {
    string indexName = 1; // Index name
    double mergeMaxMBPerSec = 2; // Rate limit merges to at most this many MB/sec
    double nrtCachingDirectoryMaxMergeSizeMB = 3; // Largest merged segment size to cache in RAMDirectory, default: 5.0MB
    double nrtCachingDirectoryMaxSizeMB = 4; // Largest overall size for all files cached in NRTCachingDirectory; set to -1 to disable NRTCachingDirectory default: 60.0MB
    int32 concurrentMergeSchedulerMaxThreadCount = 5; // How many merge threads to allow at once
    int32 concurrentMergeSchedulerMaxMergeCount = 6; // Maximum backlog of pending merges before indexing threads are stalled
    SortFields indexSort = 7; // Index time sorting; can only be written once", SearchHandler.SORT_TYPE
    bool indexVerbose = 8; // Turn on IndexWriter's infoStream (to stdout)
    bool indexMergeSchedulerAutoThrottle = 9; // Turn on/off the merge scheduler's auto throttling
    string normsFormat = 10; // Which NormsFormat should be used for all indexed fields. default: Lucene80NormsFormat
    // Base Directory implementation to use (NRTCachingDirectory will wrap this) either one of the core implementations (FSDirectory, MMapDirectory, NIOFSDirectory, SimpleFSDirectory, RAMDirectory (for temporary indices!) or a fully qualified path to a Directory implementation that has a public constructor taking a single File argument default: FSDirectory
    string directory = 11;

}

/* Settings Response returned from Server */
message SettingsResponse {
    string response = 1;
}

/* Start the index */
message StartIndexRequest {
    string indexName = 1; //index name
    Mode mode = 2; //Standalone, NRT primary or replica mode to start this index.
    int64 primaryGen = 3; //primary, the generation of this primary (should increment each time a new primary starts for this index)
    string primaryAddress = 4; //replica, the IP address or host name of the remote primary
    int32 port = 5; //replica, the TCP port of the remote primary
    RestoreIndex restore = 6; // restore index from backup
}

enum Mode {
    STANDALONE = 0;
    PRIMARY = 1;
    REPLICA = 2;
}

message StartIndexResponse {
    int32 maxDoc = 1; //one greater than the largest possible document number
    int32 numDocs = 2; //the number of documents in this index.
    string segments = 3; //string representation of the IndexReader implementation
    double startTimeMS = 4; //time taken to start the index
}

message AddDocumentRequest {
    string indexName = 1; //name of the index
    //we use this wrapper object to represent each field as a multivalued field.
    message MultiValuedField {
        repeated string value = 1; //list of values for this field
        //Facet paths/hierarchy to bucket these values by, if indexed field is of type Facet.HIERARCHY
        repeated FacetHierarchyPath faceHierarchyPaths = 2;
    }
    map<string, MultiValuedField> fields = 3; //map of field name to a list of string values.
}

message FacetHierarchyPath {
    repeated string value = 1;
}

message AddDocumentResponse {
    string genId = 1;
}

message RefreshRequest {
    string indexName = 1; //index name to be refreshed
}

message RefreshResponse {
    double refreshTimeMS = 1; //time taken in milliseconds to refresh the index
}

message CommitRequest {
    string indexName = 1; //index to commit
}

message CommitResponse {
    /*  sequence number of the last operation in the commit.  All sequence numbers less than this value
    will be reflected in the commit, and all others will not.*/
    int64 gen = 1;
}

message StatsRequest {
    string indexName = 1; //retrieve stats of the index
}

message StatsResponse {
    int32 ord = 1; //shard ordinal
    /* The total number of docs in this index, including docs not yet flushed (still in the RAM buffer),
    not counting deletions.*/
    int32 maxDoc = 2;
    /**
     * The total number of docs in this index, including
     * docs not yet flushed (still in the RAM buffer), and
     * including deletions. NOTE: buffered deletions
     * are not counted.  If you really need these to be
     * counted you should call {@link IndexWriter#commit()} first.
     */
    int32 numDocs = 3;
    int64 dirSize = 4; //size of the this indexDir
    string state = 5; //state of the index
    Taxonomy taxonomy = 6; //Taxonomy(facets) stats
    repeated Searcher searchers = 7; //Searcher stats
    Searcher currentSearcher = 8; //Current Searcher stats
}

message Taxonomy {
    int32 numOrds = 1; //number of docs in this taxonomy reader
    string segments = 2; //string representation of segments
}

message Searcher {
    /* the version recorded in the commit that the reader opened.
    This version is advanced every time a change is made with IndexWriter.*/
    int64 version = 1;
    int32 numDocs = 2; //total number of docs in this index
    string segments = 3; //string representation of segments
    double staleAgeSeconds = 4; //how much time has passed since this searcher was the current (live) searcher
    int32 numSegments = 5; // number of segments, filled only if Searcher has StandardDirectoryReader
}

message DeleteAllDocumentsRequest {
    string indexName = 1; //index to delete all documents  from
}

message DeleteAllDocumentsResponse {
    string genId = 1; //Returns the index generation (indexGen) that reflects the deletion.
}

message DeleteIndexRequest {
    string indexName = 1; //index to delete
}

message DeleteIndexResponse {
    string ok = 1; //Returns "ok" string on  success
}

message DummyResponse {
    string ok = 1; // returns "ok" string on success
}

message StopIndexRequest {
    string indexName = 1; //index name to stop
}

message BuildSuggestRequest {
    string indexName = 1; //index name
    oneof Suggester {
        //A suggester that matches terms anywhere in the input text, not just as a prefix. (see @lucene:org:server.InfixSuggester)
        InfixSuggester infixSuggester = 2;
        // Suggester that first analyzes the surface form, adds the analyzed form to a weighted FST, and then does the same thing at lookup time (see @lucene:suggest:org.apache.lucene.search.suggest.analyzing.AnalyzingSuggester
        AnalyzingSuggester analyzingSuggester = 3;
        // Implements a fuzzy AnalyzingSuggester (see @lucene:suggest:org.apache.lucene.search.suggest.analyzing.FuzzySuggester
        FuzzySuggester fuzzySuggester = 4;
    }
    oneof Source {
        SuggestLocalSource localSource = 5;
        SuggestNonLocalSource nonLocalSource = 6;
    }
    string suggestName = 7; //Unique name for this suggest build.
}

message BuildSuggestResponse {
    int64 sizeInBytes = 1; //size in bytes in RAM if using AnalyzingSuggester
    int64 count = 2; //total number of suggester entries
}

message SuggestLookupRequest {
    string indexName = 1; //Index name
    string suggestName = 2; //Which suggester to use
    string text = 3; //Text to suggest from
    bool highlight = 4; //True if the suggestions should be highlighted (currently only works with AnalyzingInfixSuggester)
    bool allTermsRequired = 5; //If true then all terms must be found (this only applies to InfixSuggester currently)
    repeated string contexts = 6; //Which contexts to filter by
    int32 count = 7; //How many suggestions to return, default = 5
}

message SuggestLookupResponse {
    repeated OneSuggestLookupResponse results = 1; //SuggestLookup results as an array
}

message OneSuggestLookupResponse {
    oneof HighlightKey {
        /* Expert: custom Object to hold the result of a highlighted suggestion (currently only works with AnalyzingInfixSuggester) */
        SuggestLookupHighlight suggestLookupHighlight = 1;
        /* the key's text */
        string key = 2;
    }
    int64 weight = 3; //the key's weight
    string payload = 4; //the key's payload (null if not present)
}

message SuggestLookupHighlight {
    repeated OneHighlight oneHighlight = 1;
}

message OneHighlight {
    bool isHit = 1;
    string text = 2;
}

message SuggestLocalSource {
    /* Local file (to the server) to read suggestions + weights from; format is weight U+001F suggestion U+001F payload,
    one per line, with suggestion UTF-8 encoded. If this option is used then searcher, suggestField,
    weightField/Expression, payloadField should not be specified.*/
    string localFile = 1;
    bool hasContexts = 2; //True if this file provides per-suggestion contexts.
}

message SuggestNonLocalSource {
    /* Specific searcher version to use for pull suggestions to build.  There are three different ways to specify a searcher version.*/
    oneof Searcher {
        int64 indexGen = 1; //Search a generation previously returned by an indexing operation such as #addDocument.  Use this to search a non-committed (near-real-time) view of the index.
        int64 version = 2; //Search a specific searcher version.  This is typically used by follow-on searches (e.g., user clicks next page, drills down, or changes sort, etc.) to get the same searcher used by the original search.
        string snapshot = 3; //Search a snapshot previously created with #createSnapshot
    }
    string suggestField = 4; //Field (from stored documents) containing the suggestion text
    oneof Weight {
        string weightField = 5; //Numeric field (from stored documents) containing the weight
        string weightExpression = 6; //Alternative to weightField, an expression that's evaluated to the weight. Note that any fields referenced in the expression must have been indexed with sort=true

    }
    string payloadField = 7; //Optional binary or string field (from stored documents) containing the payload
    string contextField = 8; //Numeric field (from stored documents) containing the context which can be later filtered on during lookup
}

/* A suggester that matches terms anywhere in the input text, not just as a prefix. (see @lucene:org:server.InfixSuggester)*/
message InfixSuggester {
    string analyzer = 1; //Index and query analyzer
    string indexAnalyzer = 2; // Index Analyzer
    string queryAnalyzer = 3; // Query Analyzer
}

/* Suggester that first analyzes the surface form, adds the analyzed form to a weighted FST, and then does the same thing at lookup time (see @lucene:suggest:org.apache.lucene.search.suggest.analyzing.AnalyzingSuggester*/
message AnalyzingSuggester {
    string analyzer = 1; //Index and query analyzer
    string indexAnalyzer = 2; // Index Analyzer
    string queryAnalyzer = 3; // Query Analyzer
    int32 maxSurfaceFormsPerAnalyzedForm = 4; //Maximum number of surface forms to keep for a single analyzed form
    int32 maxGraphExpansions = 5; //Maximum number of graph paths to expand from the analyzed from
    bool preserveSep = 6; //True if token separators should be preserved when matching
    bool exactFirst = 7; //True if the exact match should always be returned first regardless of score
}

/* Implements a fuzzy AnalyzingSuggester (see @lucene:suggest:org.apache.lucene.search.suggest.analyzing.FuzzySuggester*/
message FuzzySuggester {
    string analyzer = 1; //Index and query analyzer
    string indexAnalyzer = 2; // Index Analyzer
    string queryAnalyzer = 3; // Query Analyzer
    int32 maxSurfaceFormsPerAnalyzedForm = 4; //Maximum number of surface forms to keep for a single analyzed form
    int32 maxGraphExpansions = 5; //Maximum number of graph paths to expand from the analyzed from
    bool preserveSep = 6; //True if token separators should be preserved when matching
    bool exactFirst = 7; //True if the exact match should always be returned first regardless of score
    int32 minFuzzyLength = 8; //Minimum key length before edits are allowed,
    int32 nonFuzzyPrefix = 9; //Key prefix where edits are not allowed,
    int32 maxEdits = 10; //Maximum number of edits for fuzzy suggestions
    bool transpositions = 11; //Whether transpositions are allowed
    bool unicodeAware = 12; //True if all edits are measured in unicode characters, not UTF-8 bytes
}

/*
Creates a snapshot in the index, which is saved point-in-time view of the last commit in the
index such that no files referenced by that snapshot will be deleted by ongoing indexing until
the snapshot is released with @releaseSnapshot.  Note that this will reference the last commit,
 so be sure to call commit first if you have pending changes that you'd like to be included in
 the snapshot.<p>This can be used for backup purposes, i.e. after creating the snapshot you can
 copy all referenced files to backup storage, and then release the snapshot once complete.
 To restore the backup, just copy all the files back and restart the server.  It can also
 be used for transactional purposes, i.e. if you sometimes need to search a specific snapshot
 instead of the current live index.<p>Creating a snapshot is very fast (does not require any
 file copying), but over time it will consume extra disk space as old segments are merged in
 the index.  Be sure to release the snapshot once you're done.  Snapshots survive shutdown
 and restart of the server.  Returns all protected filenames referenced by this snapshot:
 these files will not change and will not be deleted until the snapshot is released.
 This returns the directories and files referenced by the snapshot.
 */
message CreateSnapshotRequest {
    string indexName = 1; //name of the index to snapshot;
    bool openSearcher = 2; //Pass true if you intend to do searches against this snapshot, by passing searcher: {snapshot: X} to @search
}

message CreateSnapshotResponse {
    repeated string indexFiles = 1;
    repeated string taxonomyFiles = 2;
    repeated string stateFiles = 3;
    SnapshotId snapshotId = 4;
}

message SnapshotId {
    int64 indexGen = 1;
    int64 taxonomyGen = 2;
    int64 stateGen = 3;
}

message ReleaseSnapshotRequest {
    string indexName = 1; // name of snapshotted index to be released
    SnapshotId snapshotId = 2; //The id for this snapshot; this must have been previously created via @createSnapshot.
}

message ReleaseSnapshotResponse {
    bool success = 1; //true if successful
}

message GetAllSnapshotGenRequest {
    string indexName = 1; // name of index whose snapshotted index gens are needed
}

message GetAllSnapshotGenResponse {
    repeated int64 indexGens = 1; // list of snapshotted index gens
}

message BackupIndexRequest {
    string indexName = 1; //name of the index to backup
    string serviceName = 2; // remote storage namespace qualifier for service
    string resourceName = 3; //remote storage namespace qualifier for resource e.g. indexName
}

message BackupIndexResponse {
    string dataVersionHash = 1; //version identifier for data on s3
    string metadataVersionHash = 2; //version identifier for metadata on s3
}

message IndicesRequest {
}

message IndicesResponse {
    repeated IndexStatsResponse indicesResponse = 1; //list of IndexStatsResponse
}

message IndexStatsResponse {
    string indexName = 1; //index name
    StatsResponse statsResponse = 2; //stats for an index
}

message RestoreIndex {
    string serviceName = 1; // remote storage namespace qualifier for service
    string resourceName = 2; //remote storage namespace qualifier for resource e.g. indexName
}

message StateRequest {
    string indexName = 1; //index name
}

message StateResponse {
    string response = 1; //json string of the current index state
}

message AddReplicaRequest {
    int32 magicNumber = 1; //magic number send on all requests since these are meant for internal communication only
    string indexName = 2; //index name
    int32 replicaId = 3; //replica Id
    string hostName = 4; // replica host name
    int32 port = 5; // replica port number
}

message AddReplicaResponse {
    string ok = 1; //Returns "ok" string on  success
}

/* Holds incRef'd file level details for one point-in-time segment infos on the primary node. */
message CopyState {
    int32 infoBytesLength = 1; // infoBytes len
    bytes infoBytes = 2; //infoBytes
    int64 gen = 3; //gen
    int64 version = 4; //versiom
    FilesMetadata filesMetadata = 5; //fileMetadata
    int32 completedMergeFilesSize = 6; //completed merged files
    repeated string completedMergeFiles = 7; //names of files that finished merge
    int64 primaryGen = 8; //primary Gen
}

message FilesMetadata {
    int32 numFiles = 1; //number of files int this set
    repeated FileMetadata fileMetadata = 2; //list of metadata for each file
}

message FileMetadata {
    string fileName = 1; //file Name
    int64 len = 2; //file checksum
    int64 checksum = 3; //file checksum
    int32 headerLength = 4; //file header length;
    bytes header = 5; //file header;
    int32 footerLength = 6; //file header length;
    bytes footer = 7; //file header;
}

/** Primary invokes this on a replica to ask it to copy files */
message CopyFiles {
    int32 magicNumber = 1; //magic number send on all requests since these are meant for internal communication only
    string indexName = 2; //index name
    int64 primaryGen = 3; //primary, the generation of this primary (should increment each time a new primary starts for this index)
    FilesMetadata filesMetadata = 4; //file metadata to copy
}

/** Replica invokes this on a primary to let primary know it needs the CopyState */
message CopyStateRequest {
    int32 magicNumber = 1; //magic number send on all requests since these are meant for internal communication only
    string indexName = 2; //index name
    int32 replicaId = 3; //replica Id
}

message FilesInfo {
    int32 magicNumber = 1; //magic number send on all requests since these are meant for internal communication only
    string indexName = 2; //index name
    int32 replicaId = 3; //replica Id
    repeated FileInfo fileInfo = 4; //list of file name and offsets from where primary should start sending bytes to replica
}

message FileInfo {
    string fileName = 1; // Name of the file the replica wants primary to send
    int64 fpStart = 2; // Starting offset in the file primary should start sending bytes from:
    string indexName = 3; //index name these files belong to
}

message RawFileChunk {
    bytes content = 1; //raw contents of file
}

enum TransferStatusCode {
    Unknown = 0;
    Done = 1;
    Failed = 2;
    Ongoing = 3;
}

message HealthCheckRequest {
    bool check = 1; //healthcheck request
}

message HealthCheckResponse {
    TransferStatusCode health = 1; //enum response of healthcheck;
}

message TransferStatus {
    string Message = 1;
    TransferStatusCode Code = 2;
}

message NewNRTPoint {
    int32 magicNumber = 1; //magic number send on all requests since these are meant for internal communication only
    string indexName = 2; //index name
    int64 primaryGen = 3; //primary, the generation of this primary (should increment each time a new primary starts for this index)
    int64 version = 4; //version number when this SegmentInfos was generated
}

message IndexName {
    int32 magicNumber = 1; //magic number send on all requests since these are meant for internal communication only
    string indexName = 2; //index name
}

message SearcherVersion {
    int64 version = 1; //returns the version recorded in the commit that the reader opened.  This version is advanced every time a change is made with IndexWriter
    bool didRefresh = 2; //true if refresh happened
}

message GetNodesRequest {
    string indexName = 1; //name of the started index whose binary connections we wish to see
}

message GetNodesResponse {
    repeated NodeInfo nodes = 2; //list of NodeInfo
}

message NodeInfo {
    string hostname = 1; //name or ip address of the remote host that this node is connected to for binary replication
    int32 port = 2; //port number of the remote host that this node is connected to for binary replication
}

message DeleteByQueryRequest {
    string indexName = 1; // Index to delete documents from
    repeated Query query = 2; // Queries to match documents to be deleted
}

message ForceMergeRequest {
    string indexName = 1; // Index whose segments must be force merged
    int32 maxNumSegments = 2; // Maximum number of segments after force merge
    bool doWait = 3; // If true, waits until the force merge is completed before returning a response. Otherwise starts force merging in async and returns a response.
}

message ForceMergeResponse {
    enum Status {
        FORCE_MERGE_COMPLETED = 0;
        FORCE_MERGE_SUBMITTED = 1;
    }
    Status status = 1;
}