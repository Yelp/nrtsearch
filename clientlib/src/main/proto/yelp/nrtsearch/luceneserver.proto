/* Description of NRTSearch Service APIs and message types */
syntax = "proto3";

import "yelp/nrtsearch/search.proto";
import "yelp/nrtsearch/analysis.proto";
import "yelp/nrtsearch/suggest.proto";
import "google/api/annotations.proto";
import "google/api/httpbody.proto";
import "google/protobuf/any.proto";
import "google/protobuf/empty.proto";
import "google/protobuf/struct.proto";


option java_multiple_files = true;
option java_package = "com.yelp.nrtsearch.server.grpc";
option java_outer_classname = "LuceneServerProto";
option objc_class_prefix = "HLW";

option go_package = "github.com/Yelp/nrtsearch";

package luceneserver;

// The LuceneServer service definition.
service LuceneServer {
    /* Create an Index */
    rpc createIndex (CreateIndexRequest) returns (CreateIndexResponse) {
        option (google.api.http) = {
      post: "/v1/create_index"
      body: "*"
    };

    }
    /* Change global offline or online settings for this index. */
    rpc liveSettings (LiveSettingsRequest) returns (LiveSettingsResponse) {
        option (google.api.http) = {
      post: "/v1/live_settings"
      body: "*"
    };

    }
    /* Registers one or more fields.  Fields must be registered before they can be added in a document (via @addDocument).
      Pass a list of Fields and an indexName. Any number of fields may be registered in a single request,
      and once a field is registered it cannot be changed (write-once).
      This returns the full set of fields currently registered. */
    rpc registerFields (FieldDefRequest) returns (FieldDefResponse) {
        option (google.api.http) = {
      post: "/v1/register_fields"
      body: "*"
    };

    }

    /* Adds one or more fields.  Fields must be registered before they can be added in a document (via @addDocument).
  Pass a list of Fields and an indexName. Any number of fields may be registered in a single request,
  and once a field is registered it cannot be changed (write-once).
  This returns the full set of fields currently registered. */
    rpc updateFields (FieldDefRequest) returns (FieldDefResponse) {
        option (google.api.http) = {
      post: "/v1/update_fields"
      body: "*"
    };

    }

    /* Change global offline settings for this index.
    This returns the currently set settings; pass no settings changes to retrieve current settings.*/
    rpc settings (SettingsRequest) returns (SettingsResponse) {
        option (google.api.http) = {
      post: "/v1/settings"
      body: "*"
    };

    }
    /* Starts an index */
    rpc startIndex (StartIndexRequest) returns (StartIndexResponse) {
        option (google.api.http) = {
      post: "/v1/start_index"
      body: "*"
    };

    }
    /* Stops an index */
    rpc stopIndex (StopIndexRequest) returns (DummyResponse) {
        option (google.api.http) = {
      post: "/v1/stop_index"
      body: "*"
    };

    }
    /* Adds a stream of Documents */
    rpc addDocuments (stream AddDocumentRequest) returns (AddDocumentResponse) {
        option (google.api.http) = {
      post: "/v1/add_documents"
      body: "*"
    };

    }
    /* Refresh the latest searcher for an index */
    rpc refresh (RefreshRequest) returns (RefreshResponse) {
        option (google.api.http) = {
      post: "/v1/refresh"
      body: "*"
    };

    }
    /* Commits all pending changes to durable storage*/
    rpc commit (CommitRequest) returns (CommitResponse) {
        option (google.api.http) = {
      post: "/v1/commit"
      body: "*"
    };

    }
    /* Retrieve index statistics*/
    rpc stats (StatsRequest) returns (StatsResponse) {
        option (google.api.http) = {
      post: "/v1/stats"
      body: "*"
      additional_bindings {
        get: "/v1/stats/{indexName}"
      }
    };

    }
    /* Search */
    rpc search (SearchRequest) returns (SearchResponse) {
        option (google.api.http) = {
      post: "/v1/search"
      body: "*"
    };
    }

    /* Search V2 */
    rpc searchV2 (SearchRequest) returns (google.protobuf.Any) {
        option (google.api.http) = {
      post: "/v2/search"
      body: "*"
    };
    }

    /* Delete documents */
    rpc delete (AddDocumentRequest) returns (AddDocumentResponse) {
        option (google.api.http) = {
      post: "/v1/delete"
      body: "*"
    };

    }
    /* Delete documents matching a query */
    rpc deleteByQuery (DeleteByQueryRequest) returns (AddDocumentResponse) {
        option (google.api.http) = {
      post: "/v1/delete_by_query"
      body: "*"
    };

    }
    /* Delete all documents for index */
    rpc deleteAll (DeleteAllDocumentsRequest) returns (DeleteAllDocumentsResponse) {
        option (google.api.http) = {
      post: "/v1/delete_all"
      body: "*"
    };

    }
    /* Delete index */
    rpc deleteIndex (DeleteIndexRequest) returns (DeleteIndexResponse) {
        option (google.api.http) = {
      post: "/v1/delete_index"
      body: "*"
    };

    }

    /* Builds a new auto-suggester, loading suggestions via the provided local file path.*/
    rpc buildSuggest (BuildSuggestRequest) returns (BuildSuggestResponse) {
        option (google.api.http) = {
      post: "/v1/suggest_build"
      body: "*"
    };

    }
    /* Perform an auto-suggest lookup.*/
    rpc suggestLookup (SuggestLookupRequest) returns (SuggestLookupResponse) {
        option (google.api.http) = {
      post: "/v1/suggest_lookup"
      body: "*"
    };

    }
    /* Updates existing suggestions, if the suggester supports near-real-time changes. */
    rpc updateSuggest (BuildSuggestRequest) returns (BuildSuggestResponse) {
        option (google.api.http) = {
      post: "/v1/suggest_update"
      body: "*"
    };

    }

    /*
    Creates a snapshot in the index, which is saved point-in-time view of the last commit
    in the index such that no files referenced by that snapshot will be deleted by ongoing
    indexing until the snapshot is released with @releaseSnapshot.  Note that this will
    reference the last commit, so be sure to call commit first if you have pending changes
    that you'd like to be included in the snapshot.
    This can be used for backup purposes, i.e. after creating the snapshot you can copy
    all referenced files to backup storage, and then release the snapshot once complete.
    To restore the backup, just copy all the files back and restart the server.
    It can also be used for transactional purposes, i.e. if you sometimes need to search a
    specific snapshot instead of the current live index. Creating a snapshot is very fast
    (does not require any file copying), but over time it will consume extra disk space as
    old segments are merged in the index.  Be sure to release the snapshot once you're done.
    Snapshots survive shutdown and restart of the server.  Returns all protected filenames
    referenced by this snapshot: these files will not change and will not be deleted until
    the snapshot is released.  This returns the directories and files referenced by the snapshot.
    */
    rpc createSnapshot (CreateSnapshotRequest) returns (CreateSnapshotResponse) {
        option (google.api.http) = {
      post: "/v1/create_snapshot"
      body: "*"
    };

    }

    /* releases a snapshot previously created with @createSnapshot. */
    rpc releaseSnapshot (ReleaseSnapshotRequest) returns (ReleaseSnapshotResponse) {
        option (google.api.http) = {
      post: "/v1/release_snapshot"
      body: "*"
    };

    }

    /* Gets all unreleased index gens of snapshots previously created with @createSnapshot. */
    rpc getAllSnapshotIndexGen (GetAllSnapshotGenRequest) returns (GetAllSnapshotGenResponse) {
        option (google.api.http) = {
      get: "/v1/get_all_snapshot_index_gen/{indexName}"
    };

    }

    /* backs up a resource (index) and it associated metadata e.g. settings, schema to s3 */
    rpc backupIndex (BackupIndexRequest) returns (BackupIndexResponse) {
        option (google.api.http) = {
      post: "/v1/backup_index"
      body: "*"
    };
    }

    rpc deleteIndexBackup (DeleteIndexBackupRequest) returns (DeleteIndexBackupResponse) {
        option (google.api.http) = {
            post: "/v1/delete_index_backup"
            body: "*"
        };
    }

    /* Backup warming queries to S3 */
    rpc backupWarmingQueries (BackupWarmingQueriesRequest) returns (BackupWarmingQueriesResponse) {
        option (google.api.http) = {
      post: "/v1/backup_warming_queries"
      body: "*"
    };
    }

    /* Gets the state of a started index, includes settings, live_settings, search schema, suggest schema */
    rpc state (StateRequest) returns (StateResponse) {
        option (google.api.http) = {
        post: "/v1/state"
        body: "*"
        additional_bindings {
            get: "/v1/state/{indexName}"
        }
    };
    }

    //GET methods
    /* healthcheck */
    rpc status (HealthCheckRequest) returns (HealthCheckResponse) {
        option (google.api.http) = {
      get: "/v1/status"
    };

    }

    /*
    Checks if a node is ready to receive traffic by checking if all the indices (which can be preloaded)
    are started. Can specify comma-separated list of index name to only check specific indices if needed.
    */
    rpc ready (ReadyCheckRequest) returns (HealthCheckResponse) {
        option (google.api.http) = {
            get: "/v1/ready"
            additional_bindings {
				get: "/v1/ready/{indexNames}"
			}
        };
    }

    /* metrics  */
    rpc metrics (google.protobuf.Empty) returns (google.api.HttpBody) {
        option (google.api.http) = {
      get: "/status/metrics"
    };
    }
    /* indices  */
    rpc indices (IndicesRequest) returns (IndicesResponse) {
        option (google.api.http) = {
      get: "/v1/indices"
    };
    }

    /*
    Forces merge policy to merge segments until there are <= maxNumSegments. The actual
    merges to be executed are determined by the MergePolicy. This call will merge those
    segments present in the index when the call started. If other threads are still
    adding documents and flushing segments, those newly created segments will not be
    merged unless you call forceMerge again.
    */
    rpc forceMerge(ForceMergeRequest) returns (ForceMergeResponse) {
        option (google.api.http) = {
      post: "/v1/force_merge"
      body: "*"
    };
    }

    /*
    Forces merging of all segments that have deleted documents. The actual merges to be
    executed are determined by the MergePolicy. For example, the default TieredMergePolicy
    will only pick a segment if the percentage of deleted docs is over 10%.
    This method first flushes a new segment (if there are indexed documents), and applies
    all buffered deletes.
    */
    rpc forceMergeDeletes(ForceMergeDeletesRequest) returns (ForceMergeDeletesResponse) {
        option (google.api.http) = {
      post: "/v1/force_merge_deletes"
      body: "*"
    };
    }
}

//The ReplicationServer service definition.
service ReplicationServer {
    /* Issued by replica on primary node when it comes up */
    rpc addReplicas (AddReplicaRequest) returns (AddReplicaResponse) {
    }
    /* Issued by replica to receive CopyState from primary */
    rpc recvCopyState (CopyStateRequest) returns (CopyState) {
    }
    /* Send a file as a stream in chunks*/
    rpc sendRawFile (stream RawFileChunk) returns (TransferStatus) {
    }
    /* Receives a file as a stream in chunks. Typically issued by replica on primary */
    rpc recvRawFile (FileInfo) returns (stream RawFileChunk) {
    }
    rpc recvRawFileV2 (stream FileInfo) returns (stream RawFileChunk) {
    }
    /* Issued by primary on replica to inform it to start copying files either pre-warming (new merged segments) or when replica comes up first time */
    rpc copyFiles (CopyFiles) returns (stream TransferStatus) {
    }
    /* Invoked externally to replica, to notify it that a new NRT point was just created on the primary */
    rpc newNRTPoint (NewNRTPoint) returns (TransferStatus) {
    }
    /** Invoked externally to primary, to make all recent index operations searchable on the primary and, once copying is done, on the replicas */
    rpc writeNRTPoint (IndexName) returns (SearcherVersion) {
    }
    /** Invoked externally to replica, to get the current Searcher version on replica.*/
    rpc getCurrentSearcherVersion (IndexName) returns (SearcherVersion) {
    }
    /** Invoked externally on primary to find the list of replica nodes this node is connected to for binary replication per index */
    rpc getConnectedNodes (GetNodesRequest) returns (GetNodesResponse) {
    }

}

/* Input to createIndex */
message CreateIndexRequest {
    string indexName = 1; // name of the index to be created. [a-zA-Z0-9]*
}

/* Response from Server to createIndex */
message CreateIndexResponse {
    string response = 1;
}

/* Input to liveSettings */
message LiveSettingsRequest {
    string indexName = 1; // name of index whose liveSettings are to be updated.
    //Longest time to wait before reopening IndexSearcher (i.e., periodic background reopen).
    double maxRefreshSec = 2;
    //Shortest time to wait before reopening IndexSearcher (i.e., when a search is waiting for a specific indexGen).
    double minRefreshSec = 3;
    //Non-current searchers older than this are pruned.
    double maxSearcherAgeSec = 4;
    //Size (in MB) of IndexWriter's RAM buffer.
    double indexRamBufferSizeMB = 5;
    //Max number of documents to add at a time.
    int32 addDocumentsMaxBufferLen = 6;
    //Maximum number of documents allowed in a parallel search slice.
    int32 sliceMaxDocs = 7;
    //Maximum number of segments allowed in a parallel search slice.
    int32 sliceMaxSegments = 8;
    //Number of virtual shards to use for this index.
    int32 virtualShards = 9;
    //Maximum sized segment to produce during normal merging
    int32 maxMergedSegmentMB = 10;
    //Number of segments per tier used by TieredMergePolicy
    int32 segmentsPerTier = 11;
    //Timeout value to used when not specified in the search request.
    double defaultSearchTimeoutSec = 12;
    //Timeout check every value to use when not specified in the search request.
    int32 defaultSearchTimeoutCheckEvery = 13;
    //Terminate after value to use when not specified in the search request.
    int32 defaultTerminateAfter = 14;
}

/* Response from Server to liveSettings */
message LiveSettingsResponse {
    string response = 1;
}

//Type of the field
enum FieldType {
    ATOM = 0; // Text that's indexed as a single token, with DOCS_ONLY and omitting norms.
    TEXT = 1; // Text that's tokenized and indexed, with the index-time analyzer.
    BOOLEAN = 2; // Boolean value.
    LONG = 3; //Long value.
    INT = 4; // Int value.
    DOUBLE = 5; //Double value.
    FLOAT = 6; // Float value.
    LAT_LON = 7; // A latitude/longitude point.
    DATE_TIME = 8; // Date and optional time.
    // TODO name this "dynamic" instead of "virtual"?
    VIRTUAL = 9; // Virtual field defined with a JavaScript expression.
    // TODO need tests for internal:
    INTERNAL = 10; //Internal field, currently only for holding indexed facets data.
    CUSTOM = 11; // Field type specified by name.
    _ID = 12; // Field which will be used as document IDs
    POLYGON = 13; // Geojson string for polygon
    OBJECT = 14; // JSON
}

//How the tokens should be indexed.
enum IndexOptions {
    DOCS_FREQS_POSITIONS = 0; // Index doc ids, term frequencies and positions.
    DOCS = 1; // Index only doc ids (for binary search).
    DOCS_FREQS = 2; // Index doc ids and term frequencies.
    DOCS_FREQS_POSITIONS_OFFSETS = 3; // Index doc ids, term frequencies, positions and offsets.
}

//Whether/how term vectors should be indexed.
enum TermVectors {
    NO_TERMVECTORS = 0; // no term vectors are indexed
    TERMS = 1; // Index terms and freqs only.
    TERMS_POSITIONS = 2; // Index terms, freqs and positions.
    TERMS_POSITIONS_OFFSETS = 3; // Index terms, freqs, positions and offsets.
    TERMS_POSITIONS_OFFSETS_PAYLOADS = 4; // Index terms, freqs, positions, offsets and payloads
}

//Whether/How this field should index facets, and how.
enum FacetType {
    NO_FACETS = 0; //No facets are indexed.
    FLAT = 1; //Facets are indexed with no hierarchy.
    HIERARCHY = 2; //Facets are indexed and are hierarchical.
    NUMERIC_RANGE = 3; //Compute facet counts for custom numeric ranges
    SORTED_SET_DOC_VALUES = 4; //Uses SortedSetDocValuesFacetCounts, which must be flat but don't require a taxonomy index
}

message Field {
    string name = 1; // name of the field
    FieldType type = 2;
    bool search = 3; // True if the value should be available for searching (or numeric range searching, for a numeric field).
    bool store = 4; // True if the value should be stored.
    bool storeDocValues = 5; // Whether to index the value into doc values.
    bool sort = 6; // True if the value should be indexed into doc values for sorting.
    bool tokenize = 7; // True if the value should be tokenized.
    bool group = 8; // True if the value should be indexed into doc values for grouping.
    bool multiValued = 9; // True if this field may sometimes have more than one value.
    bool highlight = 10; // True if the value should be indexed for highlighting.
    bool omitNorms = 11; // True if norms are omitted.
    string dateTimeFormat = 12; // Format string used to parse datetime fields, supported values are: 1) DateTimeFormatter format 2) "epoch_millis" (datetime value is epoch timestamp)
    string postingsFormat = 13; // Which PostingsFormat should be used to index this field.
    string docValuesFormat = 14; // Which DocValuesFormat should be used to index this field.
    IndexOptions indexOptions = 15; //How the tokens should be indexed.
    Script script = 16; // The script definition defining a virtual field's value (only used with type=virtual).
    //TODO make analyzers message types i.e. StandardAnalyzer, EnglishAnalyzer, CustomAnalyzer etc
    Analyzer analyzer = 17; // Analyzer to use for this field during indexing and searching.
    Analyzer indexAnalyzer = 18; // Analyzer to use for this field during indexing.
    Analyzer searchAnalyzer = 19; //Analyzer to use for this field during searching.
    TermVectors termVectors = 20; // Whether/how term vectors should be indexed.
    //TODO make similarity message types i.d. DefaultSimilarity, CustomSimilarity, BM25Similarity;
    string similarity = 21; // Which Similarity implementation to use for this field.
    FacetType facet = 22; // Whether this field should index facets, and how.
    string facetIndexFieldName = 23; // "Which underlying Lucene index field is used to hold any indexed taxonomy or sorted set doc values facets
    google.protobuf.Struct additionalProperties = 24; // Additional info needed to configure field, used for CUSTOM types.
    google.protobuf.Struct similarityParams = 25; // Parameters for similarity implementation.
    repeated Field childFields = 26; // Child fields accessible by dot notation, index same data as parent

    bool eagerGlobalOrdinals = 27; // Compute facet global ordinals for this field up front, otherwise this is done lazily on first query. Currently only for SORTED_SET_DOC_VALUES facet type.
    bool nestedDoc = 28; // True if object data should be indexed as child documents
    // If field based global ordinals should be built up front, otherwise this is done lazily on first access. Currently only for fields with text doc values (TEXT/ATOM).
    bool eagerFieldGlobalOrdinals = 29;
}

/* Input to registerFields */
message FieldDefRequest {
    string indexName = 1; // name of the index against which the field is to be created
    repeated Field field = 2;
}

/* Response from Server for registerFields */
message FieldDefResponse {
    string response = 1;
}

/* Input to settings */
message SettingsRequest {
    string indexName = 1; // Index name
    double mergeMaxMBPerSec = 2; // Rate limit merges to at most this many MB/sec
    double nrtCachingDirectoryMaxMergeSizeMB = 3; // Largest merged segment size to cache in RAMDirectory, default: 5.0MB
    double nrtCachingDirectoryMaxSizeMB = 4; // Largest overall size for all files cached in NRTCachingDirectory; set to -1 to disable NRTCachingDirectory default: 60.0MB
    int32 concurrentMergeSchedulerMaxThreadCount = 5; // How many merge threads to allow at once
    int32 concurrentMergeSchedulerMaxMergeCount = 6; // Maximum backlog of pending merges before indexing threads are stalled
    SortFields indexSort = 7; // Index time sorting; can only be written once", SearchHandler.SORT_TYPE
    bool indexVerbose = 8 [deprecated = true]; // Deprecated, moved to lucene config. Turn on IndexWriter's infoStream (to stdout)
    bool indexMergeSchedulerAutoThrottle = 9; // Turn on/off the merge scheduler's auto throttling
    string normsFormat = 10; // Which NormsFormat should be used for all indexed fields. default: Lucene80NormsFormat
    // Base Directory implementation to use (NRTCachingDirectory will wrap this) either one of the core implementations (FSDirectory, MMapDirectory, NIOFSDirectory, SimpleFSDirectory, RAMDirectory (for temporary indices!) or a fully qualified path to a Directory implementation that has a public constructor taking a single File argument default: FSDirectory
    string directory = 11;

}

/* Settings Response returned from Server */
message SettingsResponse {
    string response = 1;
}

/* Start the index */
message StartIndexRequest {
    string indexName = 1; //index name
    Mode mode = 2; //Standalone, NRT primary or replica mode to start this index.
    int64 primaryGen = 3; //primary, the generation of this primary (should increment each time a new primary starts for this index)
    string primaryAddress = 4; //replica, the IP address or host name of the remote primary
    int32 port = 5; //replica, the TCP port of the remote primary
    RestoreIndex restore = 6; // restore index from backup
}

enum Mode {
    STANDALONE = 0;
    PRIMARY = 1;
    REPLICA = 2;
}

message StartIndexResponse {
    int32 maxDoc = 1; //one greater than the largest possible document number
    int32 numDocs = 2; //the number of documents in this index.
    string segments = 3; //string representation of the IndexReader implementation
    double startTimeMS = 4; //time taken to start the index
}

message AddDocumentRequest {
    string indexName = 1; //name of the index
    //we use this wrapper object to represent each field as a multivalued field.
    message MultiValuedField {
        repeated string value = 1; //list of values for this field
        //Facet paths/hierarchy to bucket these values by, if indexed field is of type Facet.HIERARCHY
        repeated FacetHierarchyPath faceHierarchyPaths = 2;
    }
    map<string, MultiValuedField> fields = 3; //map of field name to a list of string values.
}

message FacetHierarchyPath {
    repeated string value = 1;
}

message AddDocumentResponse {
    string genId = 1;
    // Unique identifier for the primary instance that processed the request
    string primaryId = 2;
}

message RefreshRequest {
    string indexName = 1; //index name to be refreshed
}

message RefreshResponse {
    double refreshTimeMS = 1; //time taken in milliseconds to refresh the index
}

message CommitRequest {
    string indexName = 1; //index to commit
}

message CommitResponse {
    /*  sequence number of the last operation in the commit.  All sequence numbers less than this value
    will be reflected in the commit, and all others will not.*/
    int64 gen = 1;
    // Unique identifier for the primary instance that processed the request
    string primaryId = 2;
}

message StatsRequest {
    string indexName = 1; //retrieve stats of the index
}

message StatsResponse {
    int32 ord = 1; //shard ordinal
    /* The total number of docs in this index, including docs not yet flushed (still in the RAM buffer),
    not counting deletions.*/
    int32 maxDoc = 2;
    /**
     * The total number of docs in this index, including
     * docs not yet flushed (still in the RAM buffer), and
     * including deletions. NOTE: buffered deletions
     * are not counted.  If you really need these to be
     * counted you should call {@link IndexWriter#commit()} first.
     */
    int32 numDocs = 3;
    int64 dirSize = 4; //size of the this indexDir
    string state = 5; //state of the index
    Taxonomy taxonomy = 6; //Taxonomy(facets) stats
    repeated Searcher searchers = 7; //Searcher stats
    Searcher currentSearcher = 8; //Current Searcher stats
}

message Taxonomy {
    int32 numOrds = 1; //number of docs in this taxonomy reader
    string segments = 2; //string representation of segments
}

message Searcher {
    /* the version recorded in the commit that the reader opened.
    This version is advanced every time a change is made with IndexWriter.*/
    int64 version = 1;
    int32 numDocs = 2; //total number of docs in this index
    string segments = 3; //string representation of segments
    double staleAgeSeconds = 4; //how much time has passed since this searcher was the current (live) searcher
    int32 numSegments = 5; // number of segments, filled only if Searcher has StandardDirectoryReader
}

message DeleteAllDocumentsRequest {
    string indexName = 1; //index to delete all documents  from
}

message DeleteAllDocumentsResponse {
    string genId = 1; //Returns the index generation (indexGen) that reflects the deletion.
}

message DeleteIndexRequest {
    string indexName = 1; //index to delete
}

message DeleteIndexResponse {
    string ok = 1; //Returns "ok" string on  success
}

message DummyResponse {
    string ok = 1; // returns "ok" string on success
}

message StopIndexRequest {
    string indexName = 1; //index name to stop
}

/*
Creates a snapshot in the index, which is saved point-in-time view of the last commit in the
index such that no files referenced by that snapshot will be deleted by ongoing indexing until
the snapshot is released with @releaseSnapshot.  Note that this will reference the last commit,
 so be sure to call commit first if you have pending changes that you'd like to be included in
 the snapshot.<p>This can be used for backup purposes, i.e. after creating the snapshot you can
 copy all referenced files to backup storage, and then release the snapshot once complete.
 To restore the backup, just copy all the files back and restart the server.  It can also
 be used for transactional purposes, i.e. if you sometimes need to search a specific snapshot
 instead of the current live index.<p>Creating a snapshot is very fast (does not require any
 file copying), but over time it will consume extra disk space as old segments are merged in
 the index.  Be sure to release the snapshot once you're done.  Snapshots survive shutdown
 and restart of the server.  Returns all protected filenames referenced by this snapshot:
 these files will not change and will not be deleted until the snapshot is released.
 This returns the directories and files referenced by the snapshot.
 */
message CreateSnapshotRequest {
    string indexName = 1; //name of the index to snapshot;
    bool openSearcher = 2; //Pass true if you intend to do searches against this snapshot, by passing searcher: {snapshot: X} to @search
}

message CreateSnapshotResponse {
    repeated string indexFiles = 1;
    repeated string taxonomyFiles = 2;
    repeated string stateFiles = 3;
    SnapshotId snapshotId = 4;
}

message SnapshotId {
    int64 indexGen = 1;
    int64 taxonomyGen = 2;
    int64 stateGen = 3;
}

message ReleaseSnapshotRequest {
    string indexName = 1; // name of snapshotted index to be released
    SnapshotId snapshotId = 2; //The id for this snapshot; this must have been previously created via @createSnapshot.
}

message ReleaseSnapshotResponse {
    bool success = 1; //true if successful
}

message GetAllSnapshotGenRequest {
    string indexName = 1; // name of index whose snapshotted index gens are needed
}

message GetAllSnapshotGenResponse {
    repeated int64 indexGens = 1; // list of snapshotted index gens
}

message BackupIndexRequest {
    string indexName = 1; //name of the index to backup
    string serviceName = 2; // remote storage namespace qualifier for service
    string resourceName = 3; //remote storage namespace qualifier for resource e.g. indexName
    bool completeDirectory = 4; // backup complete directory including all current snapshots if true (may backup corrupt segments if backup is created while indexing is happening), otherwise only backup the required segments and segment files
    bool stream = 5; // if the built tar should be directly streamed to s3, instead of being written to a file first (experimental)
}

message BackupIndexResponse {
    string dataVersionHash = 1; //version identifier for data on s3
    string metadataVersionHash = 2; //version identifier for metadata on s3
}

message BackupWarmingQueriesRequest {
    string index = 1; // Index whose warming queries to backup
    string serviceName = 2; // remote storage namespace qualifier for service
    int32 numQueriesThreshold = 3; // optional; minimum # of queries required to backup warming queries
    int32 uptimeMinutesThreshold = 4; // optional; minimum # of minutes uptime to backup warming queries
}

message BackupWarmingQueriesResponse {
}

message DeleteIndexBackupRequest {
    string indexName = 1; //name of the index to backup
    string serviceName = 2; // remote storage namespace qualifier for service
    string resourceName = 3; //remote storage namespace qualifier for resource e.g. indexName
    int32 nDays = 4; //backups older than nDays will be deleted from s3
}

message DeleteIndexBackupResponse {
    repeated string deletedResourceDataHashes = 1;     // version hashes of deleted backups from {resource}_data folder
    repeated string deletedResourceMetadataHashes = 2; // version hashes of deleted metadata of backups from {resource}_metadata folder
    repeated string deletedDataVersions = 3;           // versions of deleted backup data from _version/{resource}_data folder
    repeated string deletedMetadataVersions = 4;       // versions of deleted backup data from _version/{resource}_metadata folder
}

message IndicesRequest {
}

message IndicesResponse {
    repeated IndexStatsResponse indicesResponse = 1; //list of IndexStatsResponse
}

message IndexStatsResponse {
    string indexName = 1; //index name
    StatsResponse statsResponse = 2; //stats for an index
}

message RestoreIndex {
    string serviceName = 1; // remote storage namespace qualifier for service
    string resourceName = 2; //remote storage namespace qualifier for resource e.g. indexName
    bool deleteExistingData = 3; // delete any existing data for the index
}

message StateRequest {
    string indexName = 1; //index name
}

message StateResponse {
    string response = 1; //json string of the current index state
}

message AddReplicaRequest {
    int32 magicNumber = 1; //magic number send on all requests since these are meant for internal communication only
    string indexName = 2; //index name
    int32 replicaId = 3; //replica Id
    string hostName = 4; // replica host name
    int32 port = 5; // replica port number
}

message AddReplicaResponse {
    string ok = 1; //Returns "ok" string on  success
}

/* Holds incRef'd file level details for one point-in-time segment infos on the primary node. */
message CopyState {
    int32 infoBytesLength = 1; // infoBytes len
    bytes infoBytes = 2; //infoBytes
    int64 gen = 3; //gen
    int64 version = 4; //versiom
    FilesMetadata filesMetadata = 5; //fileMetadata
    int32 completedMergeFilesSize = 6; //completed merged files
    repeated string completedMergeFiles = 7; //names of files that finished merge
    int64 primaryGen = 8; //primary Gen
}

message FilesMetadata {
    int32 numFiles = 1; //number of files int this set
    repeated FileMetadata fileMetadata = 2; //list of metadata for each file
}

message FileMetadata {
    string fileName = 1; //file Name
    int64 len = 2; //file checksum
    int64 checksum = 3; //file checksum
    int32 headerLength = 4; //file header length;
    bytes header = 5; //file header;
    int32 footerLength = 6; //file header length;
    bytes footer = 7; //file header;
}

/** Primary invokes this on a replica to ask it to copy files */
message CopyFiles {
    int32 magicNumber = 1; //magic number send on all requests since these are meant for internal communication only
    string indexName = 2; //index name
    int64 primaryGen = 3; //primary, the generation of this primary (should increment each time a new primary starts for this index)
    FilesMetadata filesMetadata = 4; //file metadata to copy
}

/** Replica invokes this on a primary to let primary know it needs the CopyState */
message CopyStateRequest {
    int32 magicNumber = 1; //magic number send on all requests since these are meant for internal communication only
    string indexName = 2; //index name
    int32 replicaId = 3; //replica Id
}

message FilesInfo {
    int32 magicNumber = 1; //magic number send on all requests since these are meant for internal communication only
    string indexName = 2; //index name
    int32 replicaId = 3; //replica Id
    repeated FileInfo fileInfo = 4; //list of file name and offsets from where primary should start sending bytes to replica
}

message FileInfo {
    string fileName = 1; // Name of the file the replica wants primary to send
    int64 fpStart = 2; // Starting offset in the file primary should start sending bytes from:
    string indexName = 3; //index name these files belong to
    // sequence number to ack, only used for acked file copy
    int32 ackSeqNum = 4;
}

message RawFileChunk {
    bytes content = 1; //raw contents of file
    // sequence number for this chunk, only used for acked file copy
    int32 seqNum = 2;
    // if this chunk should be acked, only used for acked file copy
    bool ack = 3;
}

enum TransferStatusCode {
    Unknown = 0;
    Done = 1;
    Failed = 2;
    Ongoing = 3;
}

message HealthCheckRequest {
    bool check = 1; //healthcheck request
}

message HealthCheckResponse {
    TransferStatusCode health = 1; //enum response of healthcheck;
}

message ReadyCheckRequest {
    string indexNames = 1;
}

message TransferStatus {
    string Message = 1;
    TransferStatusCode Code = 2;
}

message NewNRTPoint {
    int32 magicNumber = 1; //magic number send on all requests since these are meant for internal communication only
    string indexName = 2; //index name
    int64 primaryGen = 3; //primary, the generation of this primary (should increment each time a new primary starts for this index)
    int64 version = 4; //version number when this SegmentInfos was generated
}

message IndexName {
    int32 magicNumber = 1; //magic number send on all requests since these are meant for internal communication only
    string indexName = 2; //index name
}

message SearcherVersion {
    int64 version = 1; //returns the version recorded in the commit that the reader opened.  This version is advanced every time a change is made with IndexWriter
    bool didRefresh = 2; //true if refresh happened
}

message GetNodesRequest {
    string indexName = 1; //name of the started index whose binary connections we wish to see
}

message GetNodesResponse {
    repeated NodeInfo nodes = 2; //list of NodeInfo
}

message NodeInfo {
    string hostname = 1; //name or ip address of the remote host that this node is connected to for binary replication
    int32 port = 2; //port number of the remote host that this node is connected to for binary replication
}

message DeleteByQueryRequest {
    string indexName = 1; // Index to delete documents from
    repeated Query query = 2; // Queries to match documents to be deleted
}

message ForceMergeRequest {
    string indexName = 1; // Index whose segments must be force merged
    int32 maxNumSegments = 2; // Maximum number of segments after force merge
    bool doWait = 3; // If true, waits until the force merge is completed before returning a response. Otherwise starts force merging in async and returns a response.
}

message ForceMergeResponse {
    enum Status {
        FORCE_MERGE_COMPLETED = 0;
        FORCE_MERGE_SUBMITTED = 1;
    }
    Status status = 1;
}

message ForceMergeDeletesRequest {
    string indexName = 1; // Index whose segments having deletes must be force merged
    bool doWait = 2; // If true, waits until the force merge is completed before returning a response. Otherwise starts force merging in async and returns a response.
}

message ForceMergeDeletesResponse {
    enum Status {
        FORCE_MERGE_DELETES_COMPLETED = 0;
        FORCE_MERGE_DELETES_SUBMITTED = 1;
    }
    Status status = 1;
}